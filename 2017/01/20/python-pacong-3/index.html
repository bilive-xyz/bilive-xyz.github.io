<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python,爬虫," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="伪造Headers为什么要伪造请求headers?有些网站不允许除浏览器以外的方式进行访问，如果识别有问题，那么站点不会响应，所以为了爬虫正常工作，我们需要设置一些Headers 的属性。
怎么得到Headers？
以豆瓣登录为例，打开登录界面，然后右键审查元素，找到Network，然后在输入框里随便输入点东西，点击登录，如图：">
<meta property="og:type" content="article">
<meta property="og:title" content="Python爬虫(三) -- urllib库的高级用法">
<meta property="og:url" content="http://bilive.xyz/2017/01/20/python-pacong-3/index.html">
<meta property="og:site_name" content="Surface HexoBlog">
<meta property="og:description" content="伪造Headers为什么要伪造请求headers?有些网站不允许除浏览器以外的方式进行访问，如果识别有问题，那么站点不会响应，所以为了爬虫正常工作，我们需要设置一些Headers 的属性。
怎么得到Headers？
以豆瓣登录为例，打开登录界面，然后右键审查元素，找到Network，然后在输入框里随便输入点东西，点击登录，如图：">
<meta property="og:image" content="http://ojo1aoetm.bkt.clouddn.com/Image%201.png">
<meta property="og:image" content="http://ojo1aoetm.bkt.clouddn.com/Image%202.png">
<meta property="og:updated_time" content="2017-01-20T11:08:05.567Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python爬虫(三) -- urllib库的高级用法">
<meta name="twitter:description" content="伪造Headers为什么要伪造请求headers?有些网站不允许除浏览器以外的方式进行访问，如果识别有问题，那么站点不会响应，所以为了爬虫正常工作，我们需要设置一些Headers 的属性。
怎么得到Headers？
以豆瓣登录为例，打开登录界面，然后右键审查元素，找到Network，然后在输入框里随便输入点东西，点击登录，如图：">
<meta name="twitter:image" content="http://ojo1aoetm.bkt.clouddn.com/Image%201.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6370918075792885000',
      author: 'Surface'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://bilive.xyz/2017/01/20/python-pacong-3/"/>





  <title> Python爬虫(三) -- urllib库的高级用法 | Surface HexoBlog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Surface HexoBlog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <h1 class="site-subtitle" itemprop="description">攻克从入门到放弃系列</h1>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://bilive.xyz/2017/01/20/python-pacong-3/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Surface">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/uploads/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Surface HexoBlog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Surface HexoBlog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
            
            
              
                Python爬虫(三) -- urllib库的高级用法
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-20T11:42:20+08:00">
                2017-01-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/01/20/python-pacong-3/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/01/20/python-pacong-3/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="伪造Headers"><a href="#伪造Headers" class="headerlink" title="伪造Headers"></a>伪造Headers</h2><h3 id="为什么要伪造请求headers"><a href="#为什么要伪造请求headers" class="headerlink" title="为什么要伪造请求headers?"></a>为什么要伪造请求headers?</h3><p>有些网站不允许除浏览器以外的方式进行访问，如果识别有问题，那么站点不会响应，所以为了爬虫正常工作，我们需要设置一些Headers 的属性。</p>
<p><strong>怎么得到Headers？</strong></p>
<p>以豆瓣登录为例，打开登录界面，然后右键审查元素，找到Network，然后在输入框里随便输入点东西，点击登录，如图：<br><a id="more"></a><br><img src="http://ojo1aoetm.bkt.clouddn.com/Image%201.png" alt="douban"></p>
<p> 其中的User-Agent，它的作用是告诉服务器，用户客户端是什么浏览器，以及操作系统的信息。在某些特殊的情况下，服务器根据浏览器的不同类型，输出不同的内容。User-Agent是不可靠的，原因就是它是由客户端自己决定并发送给服务器。</p>
<p> 下面一个例子，只是说明了怎样设置hreders。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import urllib  </div><div class="line">import urllib2  </div><div class="line"> </div><div class="line">url = <span class="string">'http://www.server.com/login'</span></div><div class="line">user_agent = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0'</span>&#125;</div><div class="line">values = &#123;<span class="string">'username'</span> : <span class="string">'abc'</span>,  <span class="string">'password'</span> : <span class="string">'XXXX'</span> &#125;  </div><div class="line">headers = &#123; <span class="string">'User-Agent'</span> : user_agent &#125;  </div><div class="line">data = urllib.urlencode(values)  </div><div class="line">request = urllib2.Request(url, data, headers)  </div><div class="line">response = urllib2.urlopen(request)  </div><div class="line">page = response.read()</div></pre></td></tr></table></figure>
<p>这样，我们设置了一个headers，在构建request时传入，在请求时，就加入了headers传送，服务器若识别了是浏览器发来的请求，就会得到响应内容。</p>
<p>此外，对于网站防盗链，服务器会识别headers中的referer是不是它自己，如果不是，有的服务器不会响应，因此还可以在headers中加入referer</p>
<p>例如构建如下headers：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">headers = &#123; <span class="string">'User-Agent'</span> : <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0'</span>  ,</div><div class="line">	 <span class="string">'Referer'</span>:<span class="string">'http://www.zhihu.com/articles'</span></div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>另外headers的一些属性，下面的需要特别注意一下：</p>
<blockquote>
<p>User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求<br>Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。<br>application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用<br>application/json ： 在 JSON RPC 调用时使用<br>application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用<br>在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务</p>
</blockquote>
<p>其他的有必要的可以审查浏览器的headers内容，在构建时写入同样的数据即可</p>
<h2 id="Proxy（代理）的设置"><a href="#Proxy（代理）的设置" class="headerlink" title="Proxy（代理）的设置"></a>Proxy（代理）的设置</h2><p>urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，频率过快，它会禁止你的访问。此时可以设置一些代理服务器来帮助爬虫工作，每隔一段时间换一个代理，以此突破服务器对ip的限制。</p>
<p>下面一段代码说明了代理的设置用法：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line">enable_proxy = True</div><div class="line">proxy_handler = urllib2.ProxyHandler(&#123;<span class="string">"http"</span> : <span class="string">'http://some-proxy.com:8080'</span>&#125;)</div><div class="line">null_proxy_handler = urllib2.ProxyHandler(&#123;&#125;)</div><div class="line"><span class="keyword">if</span> enable_proxy:</div><div class="line">    opener = urllib2.build_opener(proxy_handler)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    opener = urllib2.build_opener(null_proxy_handler)</div><div class="line">urllib2.install_opener(opener)</div></pre></td></tr></table></figure>
<h2 id="URLError异常处理"><a href="#URLError异常处理" class="headerlink" title="URLError异常处理"></a>URLError异常处理</h2><h3 id="URLError"><a href="#URLError" class="headerlink" title="URLError"></a>URLError</h3><p>URLError可能产生的原因：</p>
<blockquote>
<p>网络无连接，即本机无法上网<br>连接不到特定的服务器<br>服务器不存在</p>
</blockquote>
<p>在代码中，我们需要用try-except语句来包围并捕获相应的异常，如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line"></div><div class="line">requset = urllib2.Request(<span class="string">'http://www.xxxxx.com'</span>)</div><div class="line">try:</div><div class="line">    urllib2.urlopen(request)</div><div class="line">except urllib2<span class="selector-class">.URLError</span>, e:</div><div class="line">    print e.reason</div></pre></td></tr></table></figure>
<p>利用了 urlopen方法访问了一个不存在的网址，运行结果如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[Errno <span class="number">11004</span>] getaddrinfo failed</div></pre></td></tr></table></figure>
<p>它说明了错误代号是11004，错误原因是 getaddrinfo failed</p>
<h3 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a>HTTPError</h3><p>HTTPError是URLError的子类，在你利用urlopen方法发出一个请求时，服务器上都会对应一个应答对象response，其中包含一个数字”状态码”。<br>举个例子，假如response是一个”重定向”，需定位到别的地址获取文档，urllib2将对此进行处理。<br>常用状态码如：</p>
<blockquote>
<p>200：请求成功      处理方式：获得响应的内容，进行处理<br>201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到    处理方式：爬虫中不会遇到<br>202：请求被接受，但处理尚未完成    处理方式：阻塞等待<br>301：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源    处理方式：重定向到分配的URL<br>400：非法请求     处理方式：丢弃<br>401：未授权     处理方式：丢弃<br>403：禁止     处理方式：丢弃<br>404：没有找到     处理方式：丢弃<br>500：服务器内部错误  服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器端的源代码出现错误时出现。</p>
</blockquote>
<p>其它状态码，详见<a href="http://baike.baidu.com/item/HTTP%E7%8A%B6%E6%80%81%E7%A0%81" target="_blank" rel="external">HTTP状态码</a><br>HTTPError实例产生后会有一个code属性，这就是是服务器发送的相关错误号。<br>因为urllib2可以处理重定向，也就是3开头的代号可以被处理，并且100-299范围的号码指示成功，所以只能看到400-599的错误号码。<br>下面一个例子：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line"></div><div class="line">req = urllib2.Request(<span class="string">'http://xxxx.net/xxx'</span>)</div><div class="line">try:</div><div class="line">    urllib2.urlopen(req)</div><div class="line">except urllib2<span class="selector-class">.HTTPError</span>, e:</div><div class="line">    print e<span class="selector-class">.code</span></div><div class="line">    print e.reason</div></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">403</span></div><div class="line">Forbidden</div></pre></td></tr></table></figure>
<p>错误代号是403，错误原因是Forbidden，说明服务器禁止访问。<br>如果HTTPError不能捕获异常，则应用父类URLError捕获异常，因此上面代码可以这么写：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line"></div><div class="line">req = urllib2.Request(<span class="string">'http://xxxx.net/xxx'</span>)</div><div class="line">try:</div><div class="line">    urllib2.urlopen(req)</div><div class="line">except urllib2<span class="selector-class">.HTTPError</span>, e:</div><div class="line">    print e<span class="selector-class">.code</span></div><div class="line">except urllib2<span class="selector-class">.URLError</span>, e:</div><div class="line">    print e<span class="selector-class">.reason</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">    print <span class="string">"OK"</span></div></pre></td></tr></table></figure>
<p>如果捕获到了HTTPError，则输出code，不会再处理URLError异常。如果发生的不是HTTPError，则会去捕获URLError异常，输出错误原因。<br>另外还可以加入 hasattr属性提前对属性进行判断，代码改写如下:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line"></div><div class="line">req = urllib2.Request(<span class="string">'http://xxxx.net/xxx'</span>)</div><div class="line">try:</div><div class="line">    urllib2.urlopen(req)</div><div class="line">except urllib2<span class="selector-class">.URLError</span>, e:</div><div class="line">    <span class="keyword">if</span> hasattr(e,<span class="string">"code"</span>):</div><div class="line">        print e<span class="selector-class">.code</span></div><div class="line">    <span class="keyword">if</span> hasattr(e,<span class="string">"reason"</span>):</div><div class="line">        print e<span class="selector-class">.reason</span></div><div class="line"><span class="keyword">else</span>:</div><div class="line">    print <span class="string">"OK"</span></div></pre></td></tr></table></figure>
<p>首先对异常的属性进行判断，以免出现属性输出报错的现象。</p>
<h2 id="Cookie的使用"><a href="#Cookie的使用" class="headerlink" title="Cookie的使用"></a>Cookie的使用</h2><h3 id="为什么要使用Cookie？"><a href="#为什么要使用Cookie？" class="headerlink" title="为什么要使用Cookie？"></a>为什么要使用Cookie？</h3><p>Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）<br>比如说有些网站需要登录后才能访问某个页面，在登录之前，想抓取某个页面内容是不允许的。那么可以利用urllib2库保存登录的Cookie，然后再抓取页面,，以此达到目的。</p>
<h3 id="Cookielib"><a href="#Cookielib" class="headerlink" title="Cookielib"></a>Cookielib</h3><p>cookielib模块的主要作用是提供可存储cookie的对象，以便于与urllib2模块配合使用来访问Internet资源。<br>Cookielib模块非常强大，利用此模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。<br>它们的关系：CookieJar —-派生—-&gt;FileCookieJar  —-派生—–&gt;MozillaCookieJar和LWPCookieJar</p>
<h3 id="获取Cookie保存到变量"><a href="#获取Cookie保存到变量" class="headerlink" title="获取Cookie保存到变量"></a>获取Cookie保存到变量</h3><p>首先，利用CookieJar对象实现获取cookie的功能，存储到变量中：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">import urllib2</div><div class="line">import cookielib</div><div class="line">#声明一个CookieJar对象实例来保存cookie</div><div class="line">cookie = cookielib.CookieJar()</div><div class="line">#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器</div><div class="line">handler=urllib2.HTTPCookieProcessor(cookie)</div><div class="line">#通过handler来构建opener</div><div class="line">opener = urllib2.build_opener(handler)</div><div class="line">#此处的open方法同urllib2的urlopen方法，也可以传入request</div><div class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</div><div class="line">    print <span class="string">'Name = '</span>+item<span class="selector-class">.name</span></div><div class="line">    print <span class="string">'Value = '</span>+item.value</div></pre></td></tr></table></figure>
<p>使用以上方法将cookie保存到变量中，然后打印出了cookie中的值，运行结果如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Name = BAIDUID</div><div class="line">Value = B07B663B645729F11F659C02AAE65B4C:FG=<span class="number">1</span></div><div class="line">Name = BAIDUPSID</div><div class="line">Value = B07B663B645729F11F659C02AAE65B4C</div><div class="line">Name = H_PS_PSSID</div><div class="line">Value = <span class="number">12527</span>_11076_1438_10633</div><div class="line">Name = BDSVRTM</div><div class="line">Value = <span class="number">0</span></div><div class="line">Name = BD_HOME</div><div class="line">Value = <span class="number">0</span></div></pre></td></tr></table></figure>
<h3 id="保存Cookie到文件"><a href="#保存Cookie到文件" class="headerlink" title="保存Cookie到文件"></a>保存Cookie到文件</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import cookielib</div><div class="line">import urllib2</div><div class="line"></div><div class="line">#设置保存cookie的文件，同级目录下的cookie<span class="selector-class">.txt</span></div><div class="line">filename = <span class="string">'cookie.txt'</span></div><div class="line">#声明一个MozillaCookieJar对象实例来保存cookie，之后写入文件</div><div class="line">cookie = cookielib.MozillaCookieJar(filename)</div><div class="line">#利用urllib2库的HTTPCookieProcessor对象来创建cookie处理器</div><div class="line">handler = urllib2.HTTPCookieProcessor(cookie)</div><div class="line">#通过handler来构建opener</div><div class="line">opener = urllib2.build_opener(handler)</div><div class="line">#创建一个请求，原理同urllib2的urlopen</div><div class="line">response = opener.open(<span class="string">"http://www.baidu.com"</span>)</div><div class="line">#保存cookie到文件</div><div class="line">cookie.save(ignore_discard=True, ignore_expires=True)</div></pre></td></tr></table></figure>
<p>关于最后save方法的两个参数，官方解释如下：</p>
<blockquote>
<p>ignore_discard: save even cookies set to be discarded.<br>ignore_expires: save even cookies that have expiredThe file is overwritten if it already exists</p>
</blockquote>
<p>由此可见，ignore_discard的意思是即使cookies将被丢弃也将它保存下来，ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入，在这里，我们将这两个全部设置为True。运行结果如下：<br><img src="http://ojo1aoetm.bkt.clouddn.com/Image%202.png" alt="cookie"></p>
<h3 id="从文件中获取Cookie并访问"><a href="#从文件中获取Cookie并访问" class="headerlink" title="从文件中获取Cookie并访问"></a>从文件中获取Cookie并访问</h3><p>代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">import cookielib</div><div class="line">import urllib2</div><div class="line"></div><div class="line">#创建MozillaCookieJar实例对象</div><div class="line">cookie = cookielib.MozillaCookieJar()</div><div class="line">#从文件中读取cookie内容到变量</div><div class="line">cookie.load(<span class="string">'cookie.txt'</span>, ignore_discard=True, ignore_expires=True)</div><div class="line">#创建请求的request</div><div class="line">req = urllib2.Request(<span class="string">"http://www.baidu.com"</span>)</div><div class="line">#利用urllib2的build_opener方法创建一个opener</div><div class="line">opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))</div><div class="line">response = opener.open(req)</div><div class="line">print response.read()</div></pre></td></tr></table></figure>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/爬虫/" rel="tag"># 爬虫</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/01/20/python-pacong-2/" rel="next" title="Python爬虫(二) -- urllib库的基本使用">
                <i class="fa fa-chevron-left"></i> Python爬虫(二) -- urllib库的基本使用
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/01/20/python-pacong-4/" rel="prev" title="Python爬虫(四) -- 用urllib库模拟登录豆瓣">
                Python爬虫(四) -- 用urllib库模拟登录豆瓣 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/01/20/python-pacong-3/"
           data-title="Python爬虫(三) -- urllib库的高级用法" data-url="http://bilive.xyz/2017/01/20/python-pacong-3/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="Surface" />
          <p class="site-author-name" itemprop="name">Surface</p>
          <p class="site-description motion-element" itemprop="description">把时间浪费在美好的事物上</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#伪造Headers"><span class="nav-number">1.</span> <span class="nav-text">伪造Headers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要伪造请求headers"><span class="nav-number">1.1.</span> <span class="nav-text">为什么要伪造请求headers?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Proxy（代理）的设置"><span class="nav-number">2.</span> <span class="nav-text">Proxy（代理）的设置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#URLError异常处理"><span class="nav-number">3.</span> <span class="nav-text">URLError异常处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#URLError"><span class="nav-number">3.1.</span> <span class="nav-text">URLError</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTPError"><span class="nav-number">3.2.</span> <span class="nav-text">HTTPError</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cookie的使用"><span class="nav-number">4.</span> <span class="nav-text">Cookie的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么要使用Cookie？"><span class="nav-number">4.1.</span> <span class="nav-text">为什么要使用Cookie？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookielib"><span class="nav-number">4.2.</span> <span class="nav-text">Cookielib</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#获取Cookie保存到变量"><span class="nav-number">4.3.</span> <span class="nav-text">获取Cookie保存到变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#保存Cookie到文件"><span class="nav-number">4.4.</span> <span class="nav-text">保存Cookie到文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从文件中获取Cookie并访问"><span class="nav-number">4.5.</span> <span class="nav-text">从文件中获取Cookie并访问</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Surface</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  断简残篇 -
  <a class="theme-link" href="#">
    bilive.xyz
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"bilive"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
      
      <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
      <script src="/js/src/hook-duoshuo.js?v=5.1.0"></script>
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  










  
  

  

  

  

  


</body>
</html>
